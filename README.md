# Grid World Deep Q-Learning

这是一个使用深度Q学习解决5x5网格世界问题的实现。

## 问题描述

在5×5网格世界中，智能体需要学习从起点到达目标点的最优路径。环境包含以下特点：
- 起点位于(0,0)
- 目标点奖励为1
- 禁止区域奖励为-1
- 其他区域奖励为0
- 可用动作：上、下、左、右、不动

奖励矩阵：

```
[0, 0, 0, 0, 0]
[0, -1, -1, 0, 0]
[0, 0, -1, 0, 0]
[0, -1, 1, -1, 0]
[0, -1, 0, 0, 0]
```

设置：

- `r_forbidden = r_boundary = -10` (碰到边界或禁区的奖励)
- `r_target = 1` (到达目标区域的奖励)
- `r_otherstep = 0` (其他步的奖励)
- `γ = 0.9` (折扣因子)

可采取的动作：上下左右移动和原地不动（共 5 种动作）。

**目标：**

使用“深度 Q-learning”方法（off-policy 版本），学习每个“状态-动作对”的最优动作值，并计算获得最优贪心策略。

**训练设置：**

- 使用一个单独的 episode 来训练网络。
- episode 由 π(a|s) = 0.2 的探索性行为策略生成，即等可能采取每种动作。
- episode 长度：1000 步（回放缓冲区中有 1000 个样本）。
- 小批量规模：100（每次从回放缓冲区中均匀抽取 100 个样本）。
- 神经网络结构：
  - 一个隐藏层的浅层神经网络作为 q(s, a, w) 的非线性函数表示模型。
  - 隐藏层有 100 个神经元。
  - 神经网络有 3 个输入和 1 个输出：
    - 前 2 个输入是状态的归一化行索引和列索引。
    - 第 3 个输入是归一化的动作索引。
    - 输出是估计的动作值。

**输出要求：**

a)  **1000 步 episode 的轨迹可视化：**

- 在 5x5 网格世界中输出 1000 步 episode 的轨迹。
- 禁区标黄色，目标区域标蓝色，其他格子不上色。
- 轨迹用绿线表示，以颜色深浅和粗细区分走过的频次，走过越多次轨迹越粗颜色越深。

b)  **最终策略图：**

- 在 5x5 网格世界中输出获得的最终策略图。
- 用箭头表示去不同的方向，圆圈表示原地不动。

c)  **TD 误差/损失函数变化图：**

- 输出 TD 误差/损失函数随迭代次数变化的图像。

d)  **状态估计误差变化图：**

- 输出状态估计误差（State Value Error）随迭代次数变化的图像。

e)  **100 步 episode 的结果：**

- 如果只使用 100 个步的单个 episode，输出 a、b、c、d 的相应结果。

**关键点总结：**

- **深度 Q-learning (off-policy)：** 使用经验回放和目标网络等技术。
- **探索策略：** ε-greedy 策略的变体，所有动作等概率选择 (π(a|s) = 0.2)。
- **神经网络输入：** 归一化的状态（行、列索引）和动作索引。
- **神经网络输出：** 估计的 Q 值。
- **可视化：** 轨迹、策略图、TD 误差和状态估计误差变化曲线。
- **对比：** 1000 步 episode 和 100 步 episode 的结果对比。



# 安装

pip install numpy torch matplotlib seaborn



如果想用gpu

pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124

```
# 在文件开头添加设备选择
# device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
```